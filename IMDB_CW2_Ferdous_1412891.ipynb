{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#import method releated to evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "#classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#for graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the text data \"movie_data_cat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'movie_data_cat.csv'\n",
    "#df = pd.read_csv(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating mini IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"movie_data_cat.csv\")\n",
    "mn = np.random.rand(len(df)) < 0.1\n",
    "df_mini_movie_data_cat = df[mn]\n",
    "\n",
    "print(len(df_mini_movie_data_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use index=false so pandas dont create a extra index column\n",
    "df_mini_movie_data_cat.to_csv('mini_movie_data_cat.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the text data \"mini_movie_data_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mini_movie_data_cat.csv'\n",
    "df = pd.read_csv(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What else can you say about this movie,except ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The story is extremely unique.It's about these...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unlike some movies which you can wonder around...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I just found the entire 3 DVD set at Wal-Mart ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It's not just that the movie is lame. It's mor...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eisenstein describes his collaboration with Pr...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Being raised at the time this movie was releas...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Going for something far away from the delibera...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I recently bought the DVD, forgetting just how...       neg\n",
       "1  I've been impressed with Chavez's stance again...       pos\n",
       "2  What else can you say about this movie,except ...       neg\n",
       "3  The story is extremely unique.It's about these...       pos\n",
       "4  Unlike some movies which you can wonder around...       pos\n",
       "5  I just found the entire 3 DVD set at Wal-Mart ...       pos\n",
       "6  It's not just that the movie is lame. It's mor...       neg\n",
       "7  Eisenstein describes his collaboration with Pr...       pos\n",
       "8  Being raised at the time this movie was releas...       pos\n",
       "9  Going for something far away from the delibera...       pos"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the sentiment column happens to be categorical we can map the \"pos\" and \"neg\" classes to 0 and 1 integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'pos': 1}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(df['sentiment']))}\n",
    "\n",
    "print(class_mapping)\n",
    "\n",
    "class_labels = [x for x in class_mapping] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What else can you say about this movie,except ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The story is extremely unique.It's about these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unlike some movies which you can wonder around...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I just found the entire 3 DVD set at Wal-Mart ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It's not just that the movie is lame. It's mor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eisenstein describes his collaboration with Pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Being raised at the time this movie was releas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Going for something far away from the delibera...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I recently bought the DVD, forgetting just how...          0\n",
       "1  I've been impressed with Chavez's stance again...          1\n",
       "2  What else can you say about this movie,except ...          0\n",
       "3  The story is extremely unique.It's about these...          1\n",
       "4  Unlike some movies which you can wonder around...          1\n",
       "5  I just found the entire 3 DVD set at Wal-Mart ...          1\n",
       "6  It's not just that the movie is lame. It's mor...          0\n",
       "7  Eisenstein describes his collaboration with Pr...          1\n",
       "8  Being raised at the time this movie was releas...          1\n",
       "9  Going for something far away from the delibera...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the mapping dictionary to transform the class labels into integers\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(class_mapping)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploreing various cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First off, anyone looking for meaningful \"outcome oriented\" cinema that packs some sort of social message with meaningful performances and soul searching dialog spoken by dedicated, emotive, heartfelt thespians, please leave now. You are wasting your time and life is short, go see the new Brangelina Jolie movie, have a good cry, go out & buy a hybrid car or throw away your conflict diamonds if that will make you feel better, and leave us alone.<br /><br />Don\\'t let the door hit you on the way out either. THE INCREDIBLE MELTING MAN is a grade B minus regional horror epic shot in the wastelands of Oklahoma by a young, TV friendly cast & crew, and concerns itself with an astronaut who is exposed to bizarre radiation effects, wakes up in a hospital, and finds that his body is liquefying on him as he sits there feeling like a chump. The melting man is played by one Alex Rebar, who is recognizable for about the first four minutes of the film. But once he starts oozin\\' with Rick Baker\\'s extraordinary special effects makeup he more resembles something you might find in a tin of spam before you drain off all the runny, viscous blebs of grease.<br /><br />The film has zero exposition and does not bandy about with plot points: There are a couple of scenes involving scientist types riding around on an absurd industrial conveyor machine who dutifully recite a few obligatory lines about the effects of radiation but the movie does not care, really. It\\'s a freak show and a marvelous one at that with a decidedly sick sense of humor for those who can stomach it -- One great laugh comes when the melting man stumbles upon a young girl in the forest and is so at a loss for what to do that one of his eyes pops out. Hilarious.<br /><br />The \"hero\" of the film is played by Burr DeBenning, a fascinating character actor from the golden 1970s & 80s television scene who was sort of an early model for the Kevin Spacey prototype; slightly twisted, neurotic, and one step ahead of most everyone in the room even if he looks confused. He appeared just after this movie was made in a bizarre made for TV anthology horror piece called HOUSE OF THE DEAD (or THE ALIEN ZONE) that is regarded as one of the finest movies ever made in Oklahoma, which is where I suspect this film was made as well. The arid, cold looking rural midwestern landscapes are certainly the same, and the creek that one unfortunate fly fisher chooses for his afternoon of sport appears to be the same one that Cameron Mitchell fought off flying alien pancakes in WITHOUT WARNING ... which also had a sick sense of humor, a TV friendly cast, and some pretty outrageous gore. I definitely sense at least an aesthetic connection between the three movies, as well as THE SILENCE OF THE LAMBS which is of no surprise considering that director Jonathan Demme is a part of MELTING MAN\\'s cast.<br /><br />Essentially, as others have pointed out, this is a 1950s B movie plot updated for later 1970s era special effects & the inevitable boobs. The movie it probably borrows most of it\\'s ideas from is PHANTOM FROM SPACE with Peter Graves as an astronaut who also returns to Earth after being exposed to funky radiation effects that set him off on a killing spree. One of the things that I actually admire about the film is that absolutely no regard is given for the melting man\\'s motivations: He simply goes on a rampage and the movie\\'s drama comes from wondering if he\\'s going to fall to pieces before certain characters fall victim to his madness. The budget for the film is also delightfully low and every dime spent on it is up there on the screen, Rick Baker\\'s disgusting effects getting the lion\\'s share of whatever was spent on this.<br /><br />Sick, disgusting fun best enjoyed with a crowd of friends and plenty of beer. Why can\\'t people have made more movies like these? <br /><br />8/10'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1200, 'review']#[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What else can you say about this movie,except ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The story is extremely unique.It's about these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unlike some movies which you can wonder around...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I recently bought the DVD, forgetting just how...          0\n",
       "1  I've been impressed with Chavez's stance again...          1\n",
       "2  What else can you say about this movie,except ...          0\n",
       "3  The story is extremely unique.It's about these...          1\n",
       "4  Unlike some movies which you can wonder around...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regular expressions to clean up the text\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove all html markup\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons\n",
    "    \n",
    "    # remove the non-word chars '[\\W]+'\n",
    "    # append the emoticons to end \n",
    "    #convert all to lowercase\n",
    "    # remove nose char for consistency\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', '')) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the clean data preprocessor to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply the preprocessor to the entire dataframe (i.e. column review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessor to the entire dataframe (i.e. column review)\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i ve been impressed with chavez s stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what else can you say about this movie except ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the story is extremely unique it s about these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unlike some movies which you can wonder around...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  i recently bought the dvd forgetting just how ...          0\n",
       "1  i ve been impressed with chavez s stance again...          1\n",
       "2  what else can you say about this movie except ...          0\n",
       "3  the story is extremely unique it s about these...          1\n",
       "4  unlike some movies which you can wonder around...          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nadim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the stopwords if not done before (need an Internet connection)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def stop_removal(text):\n",
    "       return [w for w in text if not w in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence,', 'demonstrating', 'removal', 'stop', 'words.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence, demonstrating the removal of stop words.\"\n",
    "stopped_text = stop_removal(text.split())\n",
    "print(stopped_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic text pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The basic pipeline includes stopword removal, tokenising and stemming\n",
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "       return text.split()\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer(text)]#text.split()]\n",
    "\n",
    "\n",
    "def stop_removal(text):\n",
    "       return [w for w in text if not w in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after watching a dozen episodes i decided to give up on this show since it depicts in an unrealistic manner what is mathematical modeling in the episodes that charlie would predict the future behavior of individuals using mathematical models i thought that my profession was being joked about i am not a mathematician instead a chemical engineer but i do work a lot with mathematical models so i will try to explain to the layman why what is shown is close to make believe of fairy tales first choosing the right model to predict a situation is a demanding task charlie eppes is shown as a genius but even him would have to spend considerable time researching for a suitable model specifically for trying to guess what someone will do or where he will be in the near future individuals are erratic and haphazard there is no modeling for them isaac asimov even wrote about that in the 1950 s even if there were a model for specific kind of individual it would be a probabilistic stoichastic one meaning it has good chance of making a wrong prediction second supposing the right model for someone or a situation is found the model parameters have to be known these parameters are the constants of the equations such as the gravity acceleration 9 8 m s2 and often are not easy to determine again charlie eppes would have to be someone beyond genius to know the right parameters for the model he chooses and after the model and the parameters are chosen they would have to be tested oddly they are not and by miracle they fit exactly the situation that is being predicted third a very important aspect of modeling is almost always neglected not only by numbers but also by sci fi movies the computational effort required for solving these models try to make excel solve a complex model with many equations and variables and one will find doing a herculean job even if charlie eppes has the right software to solve his models he might be stuck with hardware that will be dreadfully slow and even with the right software hardware combination the model solution might well take days to be reached he solves them immediately i could use his computer in my research work i would be very glad as a drama it is far from being the best show the characters are somewhat stereotyped but not even remotely funny as those in big bang theory are the crimes are dull and the way charlie eppes solves them sometimes make the fbi look pretty incompetent for some layman the show might work for others the way things are handled makes it difficult to swallow '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[180, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vectorisation of text data\n",
    "Next lets prepare the data using the CountVectorizer to parse the text data into a bag-of-words model.\n",
    "Thereafter fit a sklearn calssifier.\n",
    "First we start by creating a basic train_test_split to check that the data is trasnformed correctly before setting up a comparative study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, 'review'].values\n",
    "y = df.loc[:, 'sentiment'].values\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                          random_state=42,\n",
    "                                                          test_size=0.25,\n",
    "                                                          stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method converts the text into count vector or a binary vector\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X) # Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "\n",
    "X_train = vectorizer.transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3772, 39482)\n",
      "(1258, 39482)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sice of the vocabulary or the number of features in the vector  39482\n"
     ]
    }
   ],
   "source": [
    "print('Sice of the vocabulary or the number of features in the vector ', len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apartments', 'apathetic', 'apathy', 'ape', 'apeman', 'apes', 'apex', 'aphasia', 'aphorism', 'aphorisms', 'aphrodite', 'aping', 'apke', 'aplenty', 'aplogise', 'aplomb', 'apocalypse', 'apocalyptic', 'apogee', 'apollo']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[2000:2020]) #Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip HTML and punctuation to speed up the GridSearch later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "### smaller sample\n",
    "X_train = df.loc[:2500, 'review'].values\n",
    "y_train = df.loc[:2500, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training  Classifiers on Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training KNN Classifier on Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_KNN =  KNeighborsClassifier()\n",
    "clf_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6049284578696343"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_KNN.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ANN MLP Classifier on Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_MLP =  MLPClassifier()\n",
    "clf_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8688394276629571"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_MLP.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training naive_bayes MultinomialNB Classifier on Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_NBM = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "clf_NBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NBM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8267090620031796"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NBM.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a pipeline \n",
    "In the previous example we carried out several steps involving: count vectorising; tfidf transforming and then applying this to both the train and test before fitting a classifier for preditcion. \n",
    "This pipeline of trnasformation steps and the final prediction can be carried out by setting up a pipeline.\n",
    "Instead of using the transformed vectors of X_train and Y_train ; we will use the original train and test which contained the text data i.e. text_train and text_test. These can then be sent through the transformation piepline steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6891891891891891\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.67      0.72      0.70       621\n",
      "        pos       0.71      0.66      0.68       637\n",
      "\n",
      "avg / total       0.69      0.69      0.69      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    TfidfTransformer(),\n",
    "    KNeighborsClassifier())\n",
    "\n",
    "pipeline.fit(text_train, y_train)\n",
    "y_pred = pipeline.predict(text_test)\n",
    "print('accuracy %s' % pipeline.score(text_test, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural_network MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8656597774244833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.86      0.86       621\n",
      "        pos       0.86      0.87      0.87       637\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    TfidfTransformer(),\n",
    "    MLPClassifier())\n",
    "\n",
    "pipeline.fit(text_train, y_train)\n",
    "y_pred = pipeline.predict(text_test)\n",
    "print('accuracy %s' % pipeline.score(text_test, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive_bayes MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8537360890302067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.88      0.86       621\n",
      "        pos       0.88      0.83      0.85       637\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    TfidfTransformer(),\n",
    "    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))\n",
    "\n",
    "\n",
    "pipeline.fit(text_train, y_train)\n",
    "y_pred = pipeline.predict(text_test)\n",
    "print('accuracy %s' % pipeline.score(text_test, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a gridsearch with Cross Validation\n",
    "now we will set u the grid search for there chosen classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the best params for optimal K value For KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': 1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "print(KNeighborsClassifier().get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.616\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "        {\n",
    "        'n_neighbors':  list(range(3,15))\n",
    "        }\n",
    "       ]\n",
    "\n",
    "gs = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring='accuracy')\n",
    "gs.fit(X_train,y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 12}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tfidf Grid search for  KNN with optimal params value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'tfidfvectorizer__ngram_range': [(1, 1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
    "               'tfidfvectorizer__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__max_features': [1000, 4000], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__tokenizer': [tokenizer_stemmer]}, # use a tokeniser and the stemmer \n",
    "               ]\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        #max_features=4000,\n",
    "                        min_df=7,\n",
    "                        preprocessor=None)\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(strip_accents=None, lowercase=False, min_df=7, preprocessor=None), \n",
    "                         KNeighborsClassifier(n_neighbors=12))\n",
    "\n",
    "Knn_tfidf = GridSearchCV(pipeline, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=7,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_...wski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'tfidfvectorizer__ngram_range': [(1, 1)], 'tfidfvectorizer__stop_words': [{'an', \"shan't\", 'hasn', 'who', 'by', 'its', 'y', 'am', \"don't\", 'only', 'them', \"mightn't\", 'about', 'are', 'so', 'those', 'didn', 'yourself', 'does', 'doing', 'too', 'shouldn', 'some', 'to', \"hadn't\", 'me', 'won...: [1000, 4000], 'tfidfvectorizer__tokenizer': [<function tokenizer_stemmer at 0x000001C50749B400>]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we split dataset into 2 parts, to form the test and training. \n",
    "#This will ensure that the cross validation takes place on the training data and final accuracy on the test.\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                          random_state=42,\n",
    "                                                          test_size=0.25,\n",
    "                                                          stratify=y)\n",
    "Knn_tfidf.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best tfidf params for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__max_features': 4000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': {'an', \"shan't\", 'hasn', 'who', 'by', 'its', 'y', 'am', \"don't\", 'only', 'them', \"mightn't\", 'about', 'are', 'so', 'those', 'didn', 'yourself', 'does', 'doing', 'too', 'shouldn', 'some', 'to', \"hadn't\", 'me', 'won', 't', 'ourselves', 'whom', 'd', 'not', 'the', 'for', 'll', 'mightn', 'themselves', \"you'd\", \"weren't\", 'we', 'what', 'down', 'm', 'is', 'ma', 'you', 'shan', 'or', 'through', 'weren', 'these', \"doesn't\", 'both', \"you'll\", 'she', 'he', 'on', \"you're\", 'itself', 'against', 'which', 'was', 'this', 'herself', \"couldn't\", \"didn't\", \"mustn't\", 'at', 'having', 'no', 'did', 'should', 'most', 'hadn', 'o', 'between', \"wasn't\", \"wouldn't\", 'i', 'any', \"won't\", 'where', 'now', 'wasn', 'and', 'above', 'couldn', 'her', 've', 'under', \"that'll\", 'were', 'if', \"hasn't\", 'here', 'yourselves', 'such', 'further', 'other', 'wouldn', 'our', 'ours', 'him', 'out', \"aren't\", \"haven't\", \"you've\", 'how', 'off', 'below', 'needn', 'been', 'of', 'that', 'theirs', 'with', 'when', 'few', 'from', 'again', 'each', 'until', 'then', 'your', 'but', 'yours', 'aren', 'can', 'just', \"needn't\", 'why', \"it's\", 'after', 'same', 'had', 'will', 'more', 'very', \"she's\", 'has', \"should've\", 'their', 'than', 'in', \"shouldn't\", 's', 'himself', 'there', 're', 'be', 'hers', 'all', 'nor', 'mustn', 'a', 'being', 'as', 'they', 'have', 'over', 'while', 'haven', 'because', 'into', 'ain', 'own', 'my', 'don', 'once', \"isn't\", 'up', 'it', 'doesn', 'during', 'his', 'isn', 'do', 'before', 'myself'}, 'tfidfvectorizer__tokenizer': <function tokenizer_stemmer at 0x000001C50749B400>}\n"
     ]
    }
   ],
   "source": [
    "print (Knn_tfidf.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best esimator for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.7277306468716861\n",
      "Test accuracy: 0.812\n",
      "Best Estimator :  0.8116057233704292\n"
     ]
    }
   ],
   "source": [
    "print ('Best Score :',Knn_tfidf.best_score_ )\n",
    "\n",
    "clf_KNN = Knn_tfidf.best_estimator_\n",
    "clf_KNN.fit(text_test,y_test)\n",
    "print('Test accuracy: %.3f' % clf_KNN.score(text_test, y_test))\n",
    "\n",
    "print('Best Estimator : ',Knn_tfidf.best_estimator_.score(text_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.54      0.58       621\n",
      "          1       0.61      0.69      0.65       637\n",
      "\n",
      "avg / total       0.62      0.62      0.61      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true=y_test \n",
    "y_pred=y_pred\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search for ANN MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal best params for ANN MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.870\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['identity', 'logistic', 'relu'], \n",
    "            'solver' : ['lbfgs', 'adam'],\n",
    "            'hidden_layer_sizes': [(5,),(10,),(15,)] # a single hidden layer\n",
    "        }\n",
    "       ]\n",
    "\n",
    "ANN_gs = GridSearchCV(MLPClassifier(), param_grid, cv=10, scoring='accuracy')\n",
    "ANN_gs.fit(X_train,y_train)\n",
    "y_pred = ANN_gs.predict(X_test)\n",
    "\n",
    "clf = ANN_gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'identity', 'hidden_layer_sizes': (5,), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print(ANN_gs.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up the  Grid search for ANN MLPclssifier with the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#pipe.fit(X_train, y_train)\n",
    "#y_pred = pipe.predict(X_test)\n",
    "\n",
    "param_grid = [{'tfidfvectorizer__ngram_range': [(1, 1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
    "               'tfidfvectorizer__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__max_features': [1000, 4000], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__tokenizer': [tokenizer_stemmer],}, # use a tokeniser and the stemmer \n",
    "               ]\n",
    "\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        #max_features=4000,\n",
    "                        min_df=7,\n",
    "                        preprocessor=None)\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(strip_accents=None, lowercase=False, min_df=7, preprocessor=None), \n",
    "                         MLPClassifier(solver='adam',hidden_layer_sizes=(5,),activation='identity'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ann_tfidf = GridSearchCV(pipeline, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1\n",
    "                       ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=7,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_...=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'tfidfvectorizer__ngram_range': [(1, 1)], 'tfidfvectorizer__stop_words': [{'an', \"shan't\", 'hasn', 'who', 'by', 'its', 'y', 'am', \"don't\", 'only', 'them', \"mightn't\", 'about', 'are', 'so', 'those', 'didn', 'yourself', 'does', 'doing', 'too', 'shouldn', 'some', 'to', \"hadn't\", 'me', 'won...: [1000, 4000], 'tfidfvectorizer__tokenizer': [<function tokenizer_stemmer at 0x000001C50749B400>]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we split dataset into 2 parts, to form the test and training. \n",
    "#This will ensure that the cross validation takes place on the training data and final accuracy on the test.\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                          random_state=42,\n",
    "                                                          test_size=0.25,\n",
    "                                                          stratify=y)\n",
    "Ann_tfidf.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best tfidf params for ANN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__max_features': 4000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None, 'tfidfvectorizer__tokenizer': <function tokenizer_stemmer at 0x000001C50749B400>}\n"
     ]
    }
   ],
   "source": [
    "print(Ann_tfidf.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best esimator for ANN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.8356309650053022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.000\n",
      "Best Estimator :  1.0\n"
     ]
    }
   ],
   "source": [
    "print ('Best Score :',Ann_tfidf.best_score_ )\n",
    "\n",
    "clf_Ann = Ann_tfidf.best_estimator_\n",
    "\n",
    "clf_Ann.fit(text_test,y_test)\n",
    "print('Test accuracy: %.3f' % clf_Ann.score(text_test, y_test))\n",
    "\n",
    "print('Best Estimator : ',Ann_tfidf.best_estimator_.score(text_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report for ANN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.88      0.87       621\n",
      "          1       0.88      0.87      0.87       637\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true=y_test \n",
    "y_pred=y_pred\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search for Naive Bayes MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal param values for Naive Bayes MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "print(MultinomialNB().get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "        {\n",
    "            'alpha': (1,2,3,4,5),\n",
    "            'fit_prior': (True, False),\n",
    "            'class_prior':[(1, 1), (1, 2)],\n",
    "          \n",
    "        }\n",
    "       ]\n",
    "\n",
    "NBM_gs = GridSearchCV(MultinomialNB(), param_grid, cv=10, scoring='accuracy')\n",
    "NBM_gs.fit(X_train,y_train)\n",
    "y_pred = NBM_gs.predict(X_test)\n",
    "\n",
    "clf = NBM_gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 5, 'class_prior': (1, 2), 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "print(NBM_gs.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up Grid search for Naive Bayes MB with optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'tfidfvectorizer__ngram_range': [(1, 1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
    "               'tfidfvectorizer__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__max_features': [1000, 4000], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__tokenizer': [tokenizer_stemmer]}, # use a tokeniser and the stemmer \n",
    "               ]\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        #max_features=4000,\n",
    "                        min_df=7,\n",
    "                        preprocessor=None)\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(strip_accents=None, lowercase=False, min_df=7, preprocessor=None), \n",
    "                         MultinomialNB(alpha=5.0, class_prior=(1,2), fit_prior=True))\n",
    "\n",
    "NB_tfidf = GridSearchCV(pipeline, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=7,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_... vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=5.0, class_prior=(1, 2), fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'tfidfvectorizer__ngram_range': [(1, 1)], 'tfidfvectorizer__stop_words': [{'an', \"shan't\", 'hasn', 'who', 'by', 'its', 'y', 'am', \"don't\", 'only', 'them', \"mightn't\", 'about', 'are', 'so', 'those', 'didn', 'yourself', 'does', 'doing', 'too', 'shouldn', 'some', 'to', \"hadn't\", 'me', 'won...: [1000, 4000], 'tfidfvectorizer__tokenizer': [<function tokenizer_stemmer at 0x000001C50749B400>]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we split dataset into 2 parts, to form the test and training. \n",
    "#This will ensure that the cross validation takes place on the training data and final accuracy on the test.\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                          random_state=42,\n",
    "                                                          test_size=0.25,\n",
    "                                                          stratify=y)\n",
    "NB_tfidf.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best params for Naive Bayes MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__max_features': 1000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': {'an', \"shan't\", 'hasn', 'who', 'by', 'its', 'y', 'am', \"don't\", 'only', 'them', \"mightn't\", 'about', 'are', 'so', 'those', 'didn', 'yourself', 'does', 'doing', 'too', 'shouldn', 'some', 'to', \"hadn't\", 'me', 'won', 't', 'ourselves', 'whom', 'd', 'not', 'the', 'for', 'll', 'mightn', 'themselves', \"you'd\", \"weren't\", 'we', 'what', 'down', 'm', 'is', 'ma', 'you', 'shan', 'or', 'through', 'weren', 'these', \"doesn't\", 'both', \"you'll\", 'she', 'he', 'on', \"you're\", 'itself', 'against', 'which', 'was', 'this', 'herself', \"couldn't\", \"didn't\", \"mustn't\", 'at', 'having', 'no', 'did', 'should', 'most', 'hadn', 'o', 'between', \"wasn't\", \"wouldn't\", 'i', 'any', \"won't\", 'where', 'now', 'wasn', 'and', 'above', 'couldn', 'her', 've', 'under', \"that'll\", 'were', 'if', \"hasn't\", 'here', 'yourselves', 'such', 'further', 'other', 'wouldn', 'our', 'ours', 'him', 'out', \"aren't\", \"haven't\", \"you've\", 'how', 'off', 'below', 'needn', 'been', 'of', 'that', 'theirs', 'with', 'when', 'few', 'from', 'again', 'each', 'until', 'then', 'your', 'but', 'yours', 'aren', 'can', 'just', \"needn't\", 'why', \"it's\", 'after', 'same', 'had', 'will', 'more', 'very', \"she's\", 'has', \"should've\", 'their', 'than', 'in', \"shouldn't\", 's', 'himself', 'there', 're', 'be', 'hers', 'all', 'nor', 'mustn', 'a', 'being', 'as', 'they', 'have', 'over', 'while', 'haven', 'because', 'into', 'ain', 'own', 'my', 'don', 'once', \"isn't\", 'up', 'it', 'doesn', 'during', 'his', 'isn', 'do', 'before', 'myself'}, 'tfidfvectorizer__tokenizer': <function tokenizer_stemmer at 0x000001C50749B400>}\n"
     ]
    }
   ],
   "source": [
    "print (NB_tfidf.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best esimator for Naive Bayes MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.6540296924708378\n",
      "Test accuracy: 0.63514\n",
      "Best Estimator :  0.6351351351351351\n"
     ]
    }
   ],
   "source": [
    "print ('Best Score :',NB_tfidf.best_score_ )\n",
    "\n",
    "clf_NB = NB_tfidf.best_estimator_\n",
    "clf_NB.fit(text_test,y_test)\n",
    "print('Test accuracy: %.5f' % clf_NB.score(text_test, y_test))\n",
    "\n",
    "print('Best Estimator : ',NB_tfidf.best_estimator_.score(text_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report for Naive Bayes MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.84      0.83       621\n",
      "          1       0.84      0.83      0.84       637\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true=y_test \n",
    "y_pred=y_pred\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
